{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 \n",
    "# Importing all the required libraries \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20\n",
    "# Scrape the text and labels from the dataset\n",
    "# insert script to scrape the text and labels. Ask Praneet to combine both scripts to one. TODO\n",
    "# Also, write multiple files. Plain Text, Stemmed Text, Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    19411\n",
       "True       189\n",
       "Name: English, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30\n",
    "# Number of files without English Text. TODO: Rakesh\n",
    "# Read the scraped file content into a data frame and then remove the files which have text\n",
    "# There is no English version of this document available since it was not included in the English Special Edition.\n",
    "\n",
    "_stem_text_df = pd.read_csv(\"../data/Stem_Text.csv\", encoding=\"latin-1\")\n",
    "# Remove the files without any Eglish text (feature vector)\n",
    "_stem_text_df[\"English\"] = _stem_text_df[\"Text\"].str.contains('english version document avail sinc includ english special edit', regex=True)\n",
    "_stem_text_df['English'].value_counts()\n",
    "# TRUE files do not have English Text in them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40\n",
    "# How many files have how many number of labels? From Rabi\n",
    "def count_number(labels):\n",
    "    split_labels = str(labels).split(',')\n",
    "    total_labels = len(split_labels)\n",
    "    return total_labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _label_df = pd.read_csv(\"../data/Eurovoc_Labels.csv\")\n",
    "    # Count number of labels present in each file\n",
    "    _label_df = _label_df.groupby(['Filename'])['Eurovoc_Label'].apply(', '.join).reset_index()\n",
    "    _label_df['count_of_labels'] = _label_df['Eurovoc_Label'].apply(count_number)\n",
    "    # Checking maximum count of label present\n",
    "    count_max_label = _label_df['count_of_labels'].max()\n",
    "    index_max = _label_df['count_of_labels'].idxmax()\n",
    "    print(count_max_label)\n",
    "    print(_label_df['Filename'].iloc[index_max])\n",
    "    # Top 10 files with highest number of labels\n",
    "    # max_label_files = _label_df.nlargest(10, 'count_of_labels').reset_index()\n",
    "    # plt.bar(max_label_files['Filename'], max_label_files['count_of_labels'])\n",
    "    # plt.show()\n",
    "\n",
    "    # Group by count_of_labels and create a new data-frame\n",
    "    _group_by_labels_df = _label_df.groupby('count_of_labels')['Filename'].apply(', '.join).reset_index()\n",
    "    _group_by_labels_df['count_of_Filename'] = _group_by_labels_df['Filename'].apply(count_number)\n",
    "    _group_by_labels_df = _group_by_labels_df.sort_values('count_of_Filename').reset_index()\n",
    "    _group_by_labels_df = _group_by_labels_df.drop(columns='Filename')\n",
    "    #print(_group_by_labels_df)\n",
    "\n",
    "    # Number of labels vs Number of files\n",
    "    plt.barh(_group_by_labels_df['count_of_labels'].tail(5), _group_by_labels_df['count_of_Filename'].tail(5))\n",
    "    plt.xlabel('Number of Files')\n",
    "    plt.ylabel('Number of Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Number of files vs Number of labels\n",
    "    plt.bar(_group_by_labels_df['count_of_labels'], _group_by_labels_df['count_of_Filename'])\n",
    "    plt.xlabel('Number of Labels')\n",
    "    plt.ylabel('Number of Files')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50\n",
    "# Reading from labels file and transposing them. One row each for a file. \n",
    "# Transpose using pivot table. So, we have as many columns as distinct labels. Value under a label shows whether the document has the label or not.\n",
    "_labels_df = pd.read_csv(\"../data/Eurovoc_Labels.csv\", encoding=\"latin-1\")\n",
    "_labels_df['pivotcol'] = 1\n",
    "_lbl_trnsps_df = pd.pivot_table(_labels_df,values='pivotcol',index=['Filename'],columns=['Eurovoc_Label'],aggfunc=np.sum, fill_value=0)\n",
    "#_lbl_trnsps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 55\n",
    "# Reading from labels file and transposing them. One row each for a file. \n",
    "# Aggregate labels into aggregated list format for each file\n",
    "_labels_df = _labels_df.groupby(['Filename'])['Eurovoc_Label'].apply(list).reset_index()\n",
    "type(_lbl_trnsps_df)\n",
    "type(_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60\n",
    "# merge the features and labels transposed \n",
    "df_for_stemming = _stem_text_df[_stem_text_df['English']==False]\n",
    "_stem_text_df = df_for_stemming.drop(columns='English')\n",
    "_merged_df = pd.merge(_stem_text_df, _lbl_trnsps_df, on='Filename')\n",
    "_stem_text_df = df_for_stemming.drop(columns='English')\n",
    "#_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 61\n",
    "# merge the features and array of labels\n",
    "_array_df = pd.merge(_stem_text_df, _labels_df, on='Filename')\n",
    "#_array_df.head()\n",
    "type(_array_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = _array_df.head(100)\n",
    "testdf.head()\n",
    "type(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testdf.to_csv(\"Testdf.csv\")\n",
    "test1df = pd.read_csv(\"Testdf_text_tolist.csv\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test1df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on a subset of data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(testdf.Eurovoc_Label)\n",
    "Y = multilabel_binarizer.transform(testdf.Eurovoc_Label)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(testdf.Text)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Text</th>\n",
       "      <th>Eurovoc_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32002D0182_EN_NOT.html</td>\n",
       "      <td>commiss decis 28 februari 2002 approv amend pl...</td>\n",
       "      <td>[animal plague, swine, region, Austria, wild m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32005R1522_EN_NOT.html</td>\n",
       "      <td>commiss regul ( EC ) 1522/2005 20 septemb 2005...</td>\n",
       "      <td>[barley, intervention agency, Hungary, award o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31962R0049_EN_NOT.html</td>\n",
       "      <td>regul 49 council amend date certain instrument...</td>\n",
       "      <td>[common agricultural policy, common organisati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32005D0738_EN_NOT.html</td>\n",
       "      <td>commiss decis 14 septemb 2005 clearanc account...</td>\n",
       "      <td>[EAGGF Guarantee Section, closing of accounts,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31998D0273_EN_NOT.html</td>\n",
       "      <td>commiss decis 28 januari 1998 relat proceed ar...</td>\n",
       "      <td>[concessionnaire, fine, infringement of Commun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Filename                                               Text  \\\n",
       "0  32002D0182_EN_NOT.html  commiss decis 28 februari 2002 approv amend pl...   \n",
       "1  32005R1522_EN_NOT.html  commiss regul ( EC ) 1522/2005 20 septemb 2005...   \n",
       "2  31962R0049_EN_NOT.html  regul 49 council amend date certain instrument...   \n",
       "3  32005D0738_EN_NOT.html  commiss decis 14 septemb 2005 clearanc account...   \n",
       "4  31998D0273_EN_NOT.html  commiss decis 28 januari 1998 relat proceed ar...   \n",
       "\n",
       "                                       Eurovoc_Label  \n",
       "0  [animal plague, swine, region, Austria, wild m...  \n",
       "1  [barley, intervention agency, Hungary, award o...  \n",
       "2  [common agricultural policy, common organisati...  \n",
       "3  [EAGGF Guarantee Section, closing of accounts,...  \n",
       "4  [concessionnaire, fine, infringement of Commun...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 13523)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80. this is in [8]\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_resampled, Y_tfidf_resampled = ros.fit_sample(X_tfidf, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90 this is in [9]\n",
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf_resampled, Y_tfidf_resampled, test_size=0.2, random_state=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 this is in [10]. \n",
    "# this may not be needed after all. Just plotting\n",
    "#fig = plt.figure(figsize=(20,20))\n",
    "#(ax_test, ax_train) = fig.subplots(ncols=2, nrows=1)\n",
    "#g1 = sns.barplot(x=Y.sum(axis=0), y=multilabel_binarizer.classes_, ax=ax_test)\n",
    "#g2 = sns.barplot(x=y_train_tfidf.sum(axis=0), y=multilabel_binarizer.classes_, ax=ax_train)\n",
    "#g1.set_title(\"class distribution before resampling\")\n",
    "#g2.set_title(\"class distribution in training set after resampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 110. in [11]\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/float(len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)\n",
    "\n",
    "def print_score(y_pred, clf):\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test_tfidf)))\n",
    "    print(\"Hamming score: {}\".format(hamming_score(y_pred, y_test_tfidf)))\n",
    "    print(\"---\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  MultinomialNB\n",
      "Hamming loss: 0.013888888888888888\n",
      "Hamming score: 0.0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 120. in [12]\n",
    "nb_clf = MultinomialNB()\n",
    "#sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=6, tol=None)\n",
    "#lr = LogisticRegression()\n",
    "#mn = MultinomialNB()\n",
    "clf = OneVsRestClassifier(nb_clf)\n",
    "clf.fit(x_train_tfidf, y_train_tfidf)\n",
    "y_pred = clf.predict(x_test_tfidf)\n",
    "print_score(y_pred, nb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rakesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  SGDClassifier\n",
      "Hamming loss: 0.0011574074074074073\n",
      "Hamming score: 0.9166666666666666\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#nb_clf = MultinomialNB()\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=6, tol=None)\n",
    "clf = OneVsRestClassifier(sgd)\n",
    "clf.fit(x_train_tfidf, y_train_tfidf)\n",
    "y_pred = clf.predict(x_test_tfidf)\n",
    "print_score(y_pred, sgd)\n",
    "# https://xang1234.github.io/multi-label/\n",
    "# Something to try https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get rid of the warning, use something like max_iter=5, tol=-np.infty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000)\n",
    "x2 = svd.fit_transform(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.298459  ,  0.11659759, -0.20156569, ...,  0.01261915,\n",
       "        -0.28756359,  0.07311586],\n",
       "       [ 0.33053532, -0.11709454, -0.19780074, ..., -0.00843373,\n",
       "         0.00577525, -0.01820458],\n",
       "       [ 0.35505358, -0.08997341, -0.03499974, ..., -0.00829571,\n",
       "         0.0073518 , -0.01762961],\n",
       "       ...,\n",
       "       [ 0.31851803, -0.07073645, -0.1706718 , ..., -0.00303012,\n",
       "        -0.00097718, -0.00805816],\n",
       "       [ 0.33616746, -0.15982073, -0.21002734, ...,  0.00197359,\n",
       "        -0.02447071, -0.01516797],\n",
       "       [ 0.2832784 ,  0.22831058,  0.17775419, ..., -0.00635583,\n",
       "         0.00732604,  0.00113425]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
